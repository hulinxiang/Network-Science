{"cells":[{"cell_type":"markdown","id":"d56bbcee-a893-4fcb-b5a0-f4df9b203a4c","metadata":{"id":"d56bbcee-a893-4fcb-b5a0-f4df9b203a4c"},"source":["# Assignment 2"]},{"cell_type":"markdown","id":"4f708c64-d109-4ab2-ade0-1c88264dd31c","metadata":{"id":"4f708c64-d109-4ab2-ade0-1c88264dd31c"},"source":["**Your student ID, name and course level:**"]},{"cell_type":"markdown","id":"97457a45-8d00-4f8b-977a-8868028b7f3a","metadata":{"id":"97457a45-8d00-4f8b-977a-8868028b7f3a"},"source":["- uID:\n","- Name:\n","- Course level: COMP4880 or COMP8880"]},{"cell_type":"markdown","id":"9484d3d1","metadata":{"id":"9484d3d1"},"source":["**Link for Colab:**"]},{"cell_type":"markdown","id":"12844c73","metadata":{"id":"12844c73"},"source":["- Link"]},{"cell_type":"markdown","id":"a3ae57a1","metadata":{"id":"a3ae57a1"},"source":["The file mounting system in Colab is different from running code locally. When running in Colab, please run the following code first.\n","If you have placed the assignment folder in the default \"Colab Notebooks\" folder on your Google Drive, use the provided path `Colab Notebooks/A2`. Otherwise, please modify the path to match your own."]},{"cell_type":"code","execution_count":null,"id":"00e6efd7","metadata":{"id":"00e6efd7"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/Colab Notebooks/Assignment 2')"]},{"cell_type":"code","execution_count":null,"id":"8283475a-8b33-4d77-8ba9-79e553b56da7","metadata":{"id":"8283475a-8b33-4d77-8ba9-79e553b56da7"},"outputs":[],"source":["# Imports\n","import random\n","import numpy as np\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from collections import deque\n","from scipy import stats, optimize, interpolate\n","from Utils import plot_eigs, empirical_CCDF"]},{"cell_type":"markdown","id":"e0cca5ef","metadata":{"id":"e0cca5ef"},"source":["---"]},{"cell_type":"markdown","id":"95c5ef73","metadata":{"id":"95c5ef73"},"source":["# Problem 1: Heavy-tails and Light-tails"]},{"cell_type":"markdown","id":"917e072b-7bbe-49bc-ae02-7f3b64687e2d","metadata":{"id":"917e072b-7bbe-49bc-ae02-7f3b64687e2d"},"source":["## Problem 1.1: Heavy-tail vs light-tail (1.0 points)\n","Consider Weibull distributions with parameters $(\\alpha, \\beta)$, whose probability density function $f(x)=\\alpha\\beta(\\beta x)^{\\alpha -1} e^{-(\\beta x)^\\alpha}$, and complimentary cumulative density function (CCDF) $\\bar F(x)= e^{-(\\beta x)^\\alpha}$. Consider the following three distributions with given values of $(\\alpha, \\beta)$, which one(s) are heavy-tailed, and which one(s) are light-tailed? Provide proofs for your claims.   \n","(A) $\\alpha=0.5$, $\\beta=2.0$  \n","(B) $\\alpha=1.0$, $\\beta=2.0$  \n","(C) $\\alpha=2.0$, $\\beta=2.0$  \n"]},{"cell_type":"markdown","id":"ca599be9-d10e-47b3-a06b-e13fd89304a3","metadata":{"id":"ca599be9-d10e-47b3-a06b-e13fd89304a3"},"source":["TODO"]},{"cell_type":"markdown","id":"397aa53d-5823-4b59-bb63-bcf34134444b","metadata":{"id":"397aa53d-5823-4b59-bb63-bcf34134444b"},"source":["---"]},{"cell_type":"markdown","id":"a2befb57-f97b-445e-8cac-ea302efef4c6","metadata":{"id":"a2befb57-f97b-445e-8cac-ea302efef4c6"},"source":["## Problem 1.2: Conspiracy vs catastrophe (2.0 points)"]},{"cell_type":"markdown","id":"b22f41e8-3376-41d7-b9ed-adf202b5a4a7","metadata":{"id":"b22f41e8-3376-41d7-b9ed-adf202b5a4a7"},"source":["What is the likely value of a random variable when the sum (or averages) of several i.i.d. draws are konwn?  $X_1$ and $X_2$ follow i.i.d. Weibull ditribution with parameters set (A)(B)(C) specified in question 1.1 above.\n","\n","- *(optional but recommended)* Derive the probability distribution of $X_1 + X_2$.\n","- *(required)* Derive the conditional density function $p(x_1 | x_1 + x_2 = 5)$\n","- *(required)* Plot the resulting three distributions in the same x-y plot, with the x-axis being $x_1 \\in (0, 5)$, and the y-axis being the conditional density function.\n","\n","**Note:** In this question, you are not required to evaluate the integrals (i.e. you can leave the integrals as-is). To evaluate the integrals numerically, you can use `scipy.integrate.quad`."]},{"cell_type":"markdown","id":"e37d8329-8165-4cf5-bbae-3a57f12e0af9","metadata":{"id":"e37d8329-8165-4cf5-bbae-3a57f12e0af9"},"source":["TODO"]},{"cell_type":"code","execution_count":null,"id":"7877a63c-9034-4ce5-8c75-3ce6316fa366","metadata":{"id":"7877a63c-9034-4ce5-8c75-3ce6316fa366"},"outputs":[],"source":["#TODO"]},{"cell_type":"markdown","id":"6661b66b-7e44-4f63-9bcc-b860f408e905","metadata":{"id":"6661b66b-7e44-4f63-9bcc-b860f408e905"},"source":["---"]},{"cell_type":"markdown","id":"e7609f95-5cfa-4eae-a11b-1bd013653d43","metadata":{"id":"e7609f95-5cfa-4eae-a11b-1bd013653d43"},"source":["## Problem 1.3: Hazard rate and distribution (1.0 points)"]},{"cell_type":"markdown","id":"f46c4493-9bf2-46f2-a6a8-3fb695957d28","metadata":{"id":"f46c4493-9bf2-46f2-a6a8-3fb695957d28"},"source":["Consider a random variable $x$ being the ``inter-arrival time'', or time before the next event (e.g. a bus, or a Reddit reply) arrives.  \n","1. For one given data domain, we know that the hazard rate is power-law with the form $q(t) = \\lambda t^{-\\gamma}$, where $\\lambda$ and $\\gamma$ are positive constants. What is the pdf $f(x) $ and CCDF $\\bar F(x) $? Show your derivations.\n","\n","**Hint:** Check the heavy tail distributions we have covered, which distribution has a power-law hazard rate."]},{"cell_type":"markdown","id":"5615a9c4-188a-44f8-b4d0-2cdd6c03f43f","metadata":{"id":"5615a9c4-188a-44f8-b4d0-2cdd6c03f43f"},"source":["TODO"]},{"cell_type":"markdown","id":"9f9d2899-4544-4efd-a7ca-bba5c18ac6aa","metadata":{"id":"9f9d2899-4544-4efd-a7ca-bba5c18ac6aa"},"source":["2. For another data domain, we know that $\\bar F(x) = \\xi x^{-\\eta}$, where $\\xi$ and $\\eta$ are positive constants. What is its hazard rate $q(t)$? Show your derivations.  "]},{"cell_type":"markdown","id":"d5354bdc-3270-4730-9917-612ada35605f","metadata":{"id":"d5354bdc-3270-4730-9917-612ada35605f"},"source":["TODO"]},{"cell_type":"markdown","id":"7692aff8-2d91-4903-9209-f517abd5ca70","metadata":{"id":"7692aff8-2d91-4903-9209-f517abd5ca70"},"source":["---"]},{"cell_type":"markdown","id":"69537ecd-4c65-46a1-b726-38c3d7737917","metadata":{"id":"69537ecd-4c65-46a1-b726-38c3d7737917"},"source":["## Problem 1.4: Estimating the heavy-tail distributions (2 + 0.5 points)"]},{"cell_type":"markdown","id":"c226a303-abd8-4b19-88db-6de129a6ba41","metadata":{"id":"c226a303-abd8-4b19-88db-6de129a6ba41"},"source":["In this task, you are given an array of 1D data `data`. The data follows either the Pareto or LogNormal distributions. Your task is to figure out which distribution is a better fit for the given data. We start by loading the data and having a simple visualisation as follows (simply run the following cell)."]},{"cell_type":"code","execution_count":null,"id":"c3ce1139-cbf2-4acd-9eea-c3862ea3810d","metadata":{"id":"c3ce1139-cbf2-4acd-9eea-c3862ea3810d"},"outputs":[],"source":["## Supporting Codes -- DO NOT CHANGE\n","# Reading data\n","data = np.load(\"./Data/1_4/data_Q1_4.npy\")\n","# Visualisation\n","plt.figure(figsize=(5, 3))\n","plt.xlabel(\"x\")\n","plt.ylabel(\"Density\")\n","plt.title(\"Histogram of Data (Q1.4)\")\n","plt.hist(data, bins = np.arange(10), edgecolor = 'black', density = True)\n","plt.show()"]},{"cell_type":"markdown","id":"f811eda8-aa3b-429f-86ba-513fb8908836","metadata":{"id":"f811eda8-aa3b-429f-86ba-513fb8908836"},"source":["### Task 1: Fitting the Pareto distribution (1 points).\n","\n","First, we infer the parameters of the distributions that fit our data best. We start with the Pareto distribution. Notice that the CCDF of the Pareto distribution is $\\bar{F}(x) = \\left(\\frac{x_m}{x}\\right)^\\alpha$. Then, by taking log on both sides, we have\n","$$ \\log\\left(\\bar{F}(x)\\right) =  - \\alpha \\log(x) + \\alpha \\log(x_m).$$ Hence, if we run linear regression with independent variable $\\log(x)$ and dependent variable $\\log\\left(\\bar{F}(x)\\right)$, we will be able recover the parameters $x_m$ and $\\alpha$. Your task is to implement the above method.\n","\n","Hints:\n","- For any sample $x_i$ in the `data`, you can use the empirical CCDF (i.e. number of data points that is $\\geq x_i$ divided by the total number of data) to serve as the observed CCDF $\\bar{F}(x_i)$.\n","- You can use [scipy.stats.linregress](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) to help you with the regression part.s"]},{"cell_type":"code","execution_count":null,"id":"d1051307-7e12-482b-bc47-b78e81c9ed8e","metadata":{"id":"d1051307-7e12-482b-bc47-b78e81c9ed8e"},"outputs":[],"source":["def pareto_parameter_estimation(data) -> (float, float):\n","\n","    # Placeholders for your answers\n","    xm = 0\n","    alpha = 0\n","\n","    #TODO\n","\n","    # Do not change the order of the outputs\n","    return xm,alpha"]},{"cell_type":"markdown","id":"5f7009a1-765d-4ac9-aea0-9223fe31442d","metadata":{"id":"5f7009a1-765d-4ac9-aea0-9223fe31442d"},"source":["If you implemented the above function correctly, the following plot should match with Figure 8.7 in our textbook of heavy tail distributions."]},{"cell_type":"code","execution_count":null,"id":"16cc9521-4850-4a28-8946-e91606861c97","metadata":{"id":"16cc9521-4850-4a28-8946-e91606861c97"},"outputs":[],"source":["# Supporting Cell for Plot generation\n","xm,alpha = pareto_parameter_estimation(data)\n","y = empirical_CCDF(data, data)\n","plt.xscale('log')\n","plt.yscale('log')\n","plt.scatter(data,y, s = 10 , label = \"Rank plot of the data\")\n","xaxis = np.linspace(0.3,100)\n","plt.plot(xaxis, (xm/xaxis)**alpha, c = 'r', label = \"Estimated CCDF\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"CCDF\")\n","plt.grid()\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"d5ddb295-ecf9-42dd-9a1f-35c0a4a07ace","metadata":{"id":"d5ddb295-ecf9-42dd-9a1f-35c0a4a07ace"},"source":["### Task 2: Fitting the LogNormal distribution (0.5 points).\n","Another hypothesis is that the data are actually following the LogNormal distribution. To verify this, we also want to infer the parameters $\\mu$ and $\\sigma$ of the LogNormal distribution that is the best fit for the data. Complete the following function.\n","\n","Hint: Suppose $X$ follows the LogNormal distribution with parameters $\\mu, \\sigma$, $\\log(X)$ follows the normal distribution with the same parameters."]},{"cell_type":"code","execution_count":null,"id":"2938bb5f-8d6d-4700-8e98-cf5c1119eb2e","metadata":{"id":"2938bb5f-8d6d-4700-8e98-cf5c1119eb2e"},"outputs":[],"source":["def logNormal_parameter_estimation(data) -> (float, float):\n","\n","    # Place holders for your answers\n","    mu = 0\n","    sigma = 0\n","\n","    #TODO\n","\n","    # Do not change the order of the outputs\n","    return mu, sigma\n"]},{"cell_type":"markdown","id":"50b6393d-b316-4344-871d-67150b6ed948","metadata":{"id":"50b6393d-b316-4344-871d-67150b6ed948"},"source":["### Task 3: Verifing Goodness of Fit (0.5 points).\n","\n","Let's first look at the plots of the CCDFs of the distributions you just estimated. You may use the plots to verify whether parameter estimation functions are correctly implemented and tell which distribution might be the best fit."]},{"cell_type":"code","execution_count":null,"id":"c95152a5-5b6a-47d0-bbdd-04376b342462","metadata":{"id":"c95152a5-5b6a-47d0-bbdd-04376b342462"},"outputs":[],"source":["# Supporting Codes that might be useful\n","def Pareto_CCDF(x, xm, alpha):\n","    return  (x/xm)**(-alpha)\n","def LogNormal_CCDF(x, mu, sigma):\n","    return 1 - stats.norm.cdf((np.log(x) - mu)/sigma)\n","\n","\n","xaxis = np.linspace(0.5, 5, 1000)\n","plt.plot(xaxis, empirical_CCDF(xaxis, data), label = \"Empirical\")\n","xm, alpha = pareto_parameter_estimation(data)\n","plt.plot(xaxis,Pareto_CCDF(xaxis, xm, alpha), label = \"Pareto-estimated\")\n","mu, sigma = logNormal_parameter_estimation(data)\n","plt.plot(xaxis,LogNormal_CCDF(xaxis, mu, sigma), label = \"LogNormal-estimated\")\n","plt.grid()\n","plt.legend()\n","plt.xlabel(\"x\")\n","plt.ylabel(\"CCDF\")\n","plt.title(\"CCDFs\")\n","plt.show()"]},{"cell_type":"markdown","id":"62a35ae2-c07d-482d-9c70-a6d51a496926","metadata":{"id":"62a35ae2-c07d-482d-9c70-a6d51a496926"},"source":["To be more rigorous, we could calculate the [Kolmogorov–Smirnov statistics](http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) to see which distribution among these three is the best fit for the given data. Let's implement a function calculate the K-S statistics. You can find the formula in the wikipedia page."]},{"cell_type":"code","execution_count":null,"id":"957c8a8a-1e69-478d-9757-5aaa2ac9fd0f","metadata":{"id":"957c8a8a-1e69-478d-9757-5aaa2ac9fd0f"},"outputs":[],"source":["def KSStatisticPareto(data, xm, alpha) -> float:\n","\n","    # Placeholders for your answers\n","    KSPareto = 0\n","    #TODO\n","\n","    return KSPareto\n","\n","def KSStatisticLogNormal(data, mu, sigma) -> float:\n","\n","    # Placeholders for your answers\n","    KSPareto = 0\n","    #TODO\n","\n","    return KSPareto\n","\n","# Supporting codes for printing things out\n","KSpareto = KSStatisticPareto(data, xm, alpha)\n","KSLogNormal = KSStatisticLogNormal(data, mu, sigma)\n","print(\"The K-S statistic of the data for Pareto distribution is \"+ str(KSpareto))\n","print(\"The K-S statistic of the data for LogNormal distribution is \"+ str(KSLogNormal))"]},{"cell_type":"markdown","id":"2d27f126-dba6-4ee5-9b9b-9bcf36044838","metadata":{"id":"2d27f126-dba6-4ee5-9b9b-9bcf36044838"},"source":["#### You should be able to answer which distribution is the best fit. Report your findings below:"]},{"cell_type":"markdown","id":"48b014e1-8714-4649-b9db-c37113d3f2db","metadata":{"id":"48b014e1-8714-4649-b9db-c37113d3f2db"},"source":["TODO"]},{"cell_type":"markdown","id":"f39b48dd-17b9-4dc3-bbd7-d0ec3d283bd9","metadata":{"id":"f39b48dd-17b9-4dc3-bbd7-d0ec3d283bd9"},"source":["### Task 4** (bonus question for both COMP4880/8880 students, 0.5 points)\n","You may have already observed that there exists some $m > 0$ such that all the data are in the interval $(m,\\infty]$. The data in the interval $[0,m]$ is missing. However, the LogNormal distribution is actually defined in the interval $[0,\\infty)$, which means that there's a problem of missing data in this interval. Assume that the datapoints in $[0,m]$ were erased on purpose (in contrast with the above implementation, where we assumed the data are missing by chance). And you want to recover the actual distributions based on the data you have currently. How to modify the parameter estimation method for the LogNormal distribution? Describe your answer (mathematically and/or empirically).\n","\n","**Note:**\n","- It is not required to consider the missing data problem other than the bonus task.\n","- In order to get full marks, you should state your idea mathematically, test your method with data and evaluate whether the performance of the new data fitting scheme is better."]},{"cell_type":"markdown","id":"faf5e6f3","metadata":{"id":"faf5e6f3"},"source":["---"]},{"cell_type":"markdown","id":"c82cef6f","metadata":{"id":"c82cef6f"},"source":["# Problem 2: Graph Spectrum"]},{"cell_type":"markdown","id":"ca8e8beb-2360-45ff-a8a8-f3af7da0d7ab","metadata":{"id":"ca8e8beb-2360-45ff-a8a8-f3af7da0d7ab"},"source":["## Problem 2.1: Eigen Values and Eigen Vectors (2.5 points)"]},{"cell_type":"markdown","id":"0d82883c-ead7-4778-8c50-f5ea58dd82c2","metadata":{"id":"0d82883c-ead7-4778-8c50-f5ea58dd82c2"},"source":["In this question, you are required to construct different graph structures from the data provided in the file `Data/2_1/X_data.txt`, compute their corresponding graph Laplacian matrices, and perform spectral analysis through eigen decomposition. Please follow the instructions below to complete the programming tasks and answer the questions."]},{"cell_type":"code","execution_count":null,"id":"2c933b72-a3d5-44b8-8579-525e86a8f34f","metadata":{"id":"2c933b72-a3d5-44b8-8579-525e86a8f34f"},"outputs":[],"source":["# Import Libraries\n","# You are not allowed to use any additional libraries, except \"from sklearn.neighbors import NearestNeighbors\" (not recommanded)\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from Utils import plot_eigs\n","\n","# Load Data\n","X = np.loadtxt(\"Data/2_1/X_data.txt\")\n","N = len(X)"]},{"cell_type":"markdown","id":"23f5c09e-afc3-437c-98ee-cf3be6f1b08e","metadata":{"id":"23f5c09e-afc3-437c-98ee-cf3be6f1b08e"},"source":["### Task 1: Graph Construction. (1.0 points)"]},{"cell_type":"markdown","id":"b17e3f5a-d0cc-47cc-a8a7-17624c54d4a9","metadata":{"id":"b17e3f5a-d0cc-47cc-a8a7-17624c54d4a9"},"source":["Complete the functions for constructing the following two types of graph structures:\n","- Mutual k-Nearest Neighbor Graph;\n","- Fully-connected Graph, using the Gaussian similarity function as the weight function."]},{"cell_type":"code","execution_count":null,"id":"17292488-e7d7-44da-8a28-3f93cf302489","metadata":{"id":"17292488-e7d7-44da-8a28-3f93cf302489"},"outputs":[],"source":["def gaussian_similarity(a, b, sigma):\n","    \"\"\"1D Gaussian similarity function.\"\"\"\n","    # TODO:\n"]},{"cell_type":"code","execution_count":null,"id":"b55ad6c9-1243-4a3f-8804-6a37b41cd5fc","metadata":{"id":"b55ad6c9-1243-4a3f-8804-6a37b41cd5fc"},"outputs":[],"source":["def build_knn_graph(X, k, sigma):\n","    \"\"\"\n","    Construct a mutual kNN graph and return the NxN weight matrix W.\n","\n","    Parameters\n","    ----------\n","    X : ndarray of shape (N,) or (N, d)\n","        Dataset. If one-dimensional, shape=(N, ). If multi-dimensional, can be\n","        extended for subsequent logic.\n","    k : int\n","        Number of neighbors for each point.\n","    sigma : float\n","        Parameter for the Gaussian similarity kernel.\n","    method :\n","        mutual kNN graph (an edge is created only if both sides are neighbors)\n","\n","    Returns\n","    -------\n","    W : ndarray of shape (N, N)\n","        Symmetric weight matrix.\n","    \"\"\"\n","    # TODO:\n","\n","    return W"]},{"cell_type":"code","execution_count":null,"id":"18a80592-0d66-4eb0-9a51-0f92d183ea86","metadata":{"id":"18a80592-0d66-4eb0-9a51-0f92d183ea86"},"outputs":[],"source":["def build_full_graph(X, sigma=1.0):\n","    \"\"\"\n","    Construct the fully-connected graph weight matrix W (NxN).\n","    \"\"\"\n","    # TODO:\n","\n","    return W"]},{"cell_type":"markdown","id":"c1fa1dd3-5522-4f55-a293-8896dc07a9a5","metadata":{"id":"c1fa1dd3-5522-4f55-a293-8896dc07a9a5"},"source":["### Task 2: Graph Laplacian Matrix Computation (0.2 points)"]},{"cell_type":"markdown","id":"58dd95fd-1800-49e0-afb2-ca7ec8bb6d79","metadata":{"id":"58dd95fd-1800-49e0-afb2-ca7ec8bb6d79"},"source":["Complete the functions for computing the following Laplacian matrix:\n","- Unnormalized Laplacian matrix ($L_{\\text{unnorm}}$);\n","- Random walk Laplacian matrix ($L_{\\text{rw}}$)."]},{"cell_type":"code","execution_count":null,"id":"fa4c5c4c-8cc3-4b1b-9aae-cea6821e88c4","metadata":{"id":"fa4c5c4c-8cc3-4b1b-9aae-cea6821e88c4"},"outputs":[],"source":["def compute_laplacians(W):\n","    \"\"\"\n","    Given the weight matrix W, return (L_unnorm, L_rw).\n","    \"\"\"\n","    # TODO:\n","\n","    return L_unnorm, L_rw"]},{"cell_type":"markdown","id":"85c9045d-3fdb-47d5-8e77-92d0f157acca","metadata":{"id":"85c9045d-3fdb-47d5-8e77-92d0f157acca"},"source":["### Task 3: Eigenvalue and Eigenvector (0.3 points)"]},{"cell_type":"markdown","id":"0f61867f-9482-49ea-978c-8b35c429b116","metadata":{"id":"0f61867f-9482-49ea-978c-8b35c429b116"},"source":["Complete the functions for performing eigen decomposition of the above Laplacian matrices."]},{"cell_type":"code","execution_count":null,"id":"a3e999a3-f650-4ced-82e7-b5ef947f527c","metadata":{"id":"a3e999a3-f650-4ced-82e7-b5ef947f527c"},"outputs":[],"source":["def eigen_decompose(L):\n","    \"\"\"\n","    Compute the eigenvalues and eigenvectors of L, then sort them in ascending\n","    order of eigenvalues, returning (vals, vecs).\n","    \"\"\"\n","    # TODO:\n","\n","    return vals, vecs"]},{"cell_type":"markdown","id":"a0b83188-f222-46bd-8ffb-d6bdca23151b","metadata":{"id":"a0b83188-f222-46bd-8ffb-d6bdca23151b"},"source":["### Task 4: Execution Flow (0.4 points)"]},{"cell_type":"markdown","id":"c96c75d5-3a02-4ee5-a3d2-33ec2bed4003","metadata":{"id":"c96c75d5-3a02-4ee5-a3d2-33ec2bed4003"},"source":["Please make appropriate use of the function above to construct both a mutual kNN graph and a fully connected graph from the data (with $k = 9$ for the kNN graph and $\\sigma = 1.0$ as the parameter for the Gaussian similarity function). For each of the two graphs, compute both the unnormalized Laplacian matrix and the random walk Laplacian matrix. Then, plot the first 10 eigenvalues and the first 5 eigenvectors for each Laplacian matrix (The code for the plotting part has already been provided). **Please note: Even if the function itself is correct, improper usage or incorrect function calls may still result in loss of marks. Other problems follow the same logic.**"]},{"cell_type":"code","execution_count":null,"id":"e004febe-f3bc-4de9-9d85-d2fb09e2fbed","metadata":{"id":"e004febe-f3bc-4de9-9d85-d2fb09e2fbed"},"outputs":[],"source":["k = 9\n","sigma = 1.0\n","\n","# 1) mutual kNN graph\n","# build the graph and get the eigen-values/eigen-vectors\n","# TODO:\n","\n","\n","# 2) Fully-connected graph\n","# build the graph and get the eigen-values/eigen-vectors\n","# TODO:\n","\n","\n","# Plot knn unnorm in the figure: Set the figure title to \"kNN unnorm\".\n","# You may replace `'vals_knn_unnorm'` and `'vecs_knn_unnorm'` with your own variable names. Please keep the other parts unchanged.\n","fig, axs = plt.subplots(1, 6, figsize=(18, 3))\n","plot_eigs(axs[0], axs[1:], vals_knn_unnorm, vecs_knn_unnorm, X, \"kNN unnorm\", 10, 5)\n","plt.tight_layout()\n","\n","# Plot knn rw in the figure: Set the figure title to \"kNN rw\".\n","# You may replace `'vals_knn_rw'` and `'vecs_knn_rw'` with your own variable names. Please keep the other parts unchanged.\n","fig, axs = plt.subplots(1, 6, figsize=(18, 3))\n","plot_eigs(axs[0], axs[1:], vals_knn_rw, vecs_knn_rw, X, \"kNN rw\", 10, 5)\n","plt.tight_layout()\n","\n","# Plot the full unnorm in the figure: Set the figure title to \"full unnorm\".\n","# You may replace `'vals_full_unnorm'` and `'vecs_full_unnorm'` with your own variable names. Please keep the other parts unchanged.\n","fig, axs = plt.subplots(1, 6, figsize=(18, 3))\n","plot_eigs(axs[0], axs[1:], vals_full_unnorm, vecs_full_unnorm, X, \"full unnorm\", 10, 5)\n","plt.tight_layout()\n","\n","# Plot the full rw in the figure: Set the figure title to \"full rw\".\n","# You may replace `'vals_full_rw'` and `'vecs_full_rw'` with your own variable names. Please keep the other parts unchanged.\n","fig, axs = plt.subplots(1, 6, figsize=(18, 3))\n","plot_eigs(axs[0], axs[1:], vals_full_rw, vecs_full_rw, X, \"full rw\", 10, 5)\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","id":"e392e5fd-2c32-4f09-be05-54559839c5cc","metadata":{"id":"e392e5fd-2c32-4f09-be05-54559839c5cc"},"source":["### Task 5: Analytical Questions (0.6 points)"]},{"cell_type":"markdown","id":"94a758bd-4ec6-4b74-90cd-5aa36f3e179e","metadata":{"id":"94a758bd-4ec6-4b74-90cd-5aa36f3e179e"},"source":["1. How do different graph structures (mutual kNN vs. fully-connected) affect the distribution/value of the Laplacian matrices' eigenvalues and eigenvectors?"]},{"cell_type":"markdown","id":"f924d83d-e823-4b93-9647-4addd8534fa0","metadata":{"id":"f924d83d-e823-4b93-9647-4addd8534fa0"},"source":["TODO:"]},{"cell_type":"markdown","id":"c8339e09-8b39-4ff5-87e7-455faff37782","metadata":{"id":"c8339e09-8b39-4ff5-87e7-455faff37782"},"source":["2. By analyzing the eigenvalues and eigenvectors of the kNN and fully-connected graphs you get, try to find how many clusters exist in the data from at least 2 perspectives. Do you find any contradictions? From an algorithmic perspective, what might be the cause of the results' inconsistency?"]},{"cell_type":"markdown","id":"43b035d2-665c-4f1b-9c22-14dfa267eb68","metadata":{"id":"43b035d2-665c-4f1b-9c22-14dfa267eb68"},"source":["TODO\n"]},{"cell_type":"markdown","id":"306db082-7c81-452e-a324-b7cc2a681475","metadata":{"id":"306db082-7c81-452e-a324-b7cc2a681475"},"source":["---"]},{"cell_type":"markdown","id":"c1f34fc3","metadata":{"id":"c1f34fc3"},"source":["## Problem 2.2: Clustering and Partitioning (3.0 points)"]},{"cell_type":"markdown","id":"7bac7cd7","metadata":{"id":"7bac7cd7"},"source":["In this question, you will use graph spectrum as a tool to distinguish graphs of different structures (community/cluster structure, partite structure). By reordering the nodes, visualize the adjacency matrix using a Dot Matrix Plot (similar to page 4 of the Lecture on Graph Spectrum). Please follow the instructions below to complete the programming tasks and answer the questions."]},{"cell_type":"code","execution_count":null,"id":"85d5c1c0","metadata":{"id":"85d5c1c0"},"outputs":[],"source":["# Import Libraries\n","import random\n","import numpy as np\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from collections import deque\n","from sklearn.cluster import KMeans\n","from Utils import read_graph_from_files, plot_adjacency_matrix"]},{"cell_type":"code","execution_count":null,"id":"3c829f9b","metadata":{"id":"3c829f9b"},"outputs":[],"source":["# Filenames\n","G1_nodes_file = \"Data/2_2/G1_nodes.txt\"\n","G1_edges_file = \"Data/2_2/G1_edges.txt\"\n","G2_nodes_file = \"Data/2_2/G2_nodes.txt\"\n","G2_edges_file = \"Data/2_2/G2_edges.txt\"\n","\n","# 1) Read graph\n","G1 = read_graph_from_files(G1_nodes_file, G1_edges_file)\n","G2 = read_graph_from_files(G2_nodes_file, G2_edges_file)\n","\n","# 2) Visualize the original graph\n","plot_adjacency_matrix(G1, title=\"G1 Original\")\n","plot_adjacency_matrix(G2, title=\"G2 Original\")"]},{"cell_type":"markdown","id":"993201de","metadata":{"id":"993201de"},"source":["### Task 1: Determine the type of graph structure. (1.0 points)"]},{"cell_type":"markdown","id":"4896a969","metadata":{"id":"4896a969"},"source":["Please determine whether G1 and G2 exhibit a community structure or a partite structure based on the eigenvalues of their adjacency matrix or Laplacian matrix, respectively. In this step, you only need to determine whether each graph is bipartite. You can also try to plot eigenvalues to see what the eigenvalues are (not necessarily plot all eigenvalue).\n","\n","Do not assume “it’s not bipartite, so it must be a two-cluster graph”. You need to independently implement the logic for both structural identifications based on their definitions and mathematical formulations. This means you should not directly call library functions that identify bipartite or community structures."]},{"cell_type":"code","execution_count":null,"id":"f75f5339","metadata":{"id":"f75f5339"},"outputs":[],"source":["def check_community_by_eigenvalues(G, tol=1e-3):\n","    \"\"\"\n","    Determine whether graph G has a community structure.\n","\n","    Parameters:\n","      G: NetworkX graph.\n","      tol: Tolerance for comparing.\n","\n","    Returns:\n","      has_communities: Boolean; True if more than one eigenvalue is below tol.\n","      num_communities: Predicted number of communities (equal to the number of eigenvalues < tol, use the tol give here).\n","      eigenvalues: The sorted array of eigenvalues.\n","    \"\"\"\n","    # TODO:\n","\n","\n","    if has_communities:\n","        print(\"The graph appears to have community structure with {} communities.\".format(num_communities))\n","    else:\n","        print(\"The graph does not appear to have a clear community structure.\")\n","\n","    return has_communities, num_communities, eigenvalues"]},{"cell_type":"code","execution_count":null,"id":"3dc20643","metadata":{"id":"3dc20643"},"outputs":[],"source":["def check_bipartite_by_eigenvalues(G, tol=1e-5):\n","    \"\"\"\n","    Determine whether graph G is bipartite using the spectral properties.\n","\n","    Parameters:\n","      G: NetworkX graph.\n","      tol: Tolerance for comparing (use the tol given here).\n","\n","    Returns:\n","      is_bipartite: Boolean;\n","      eigenvalues: The sorted array of eigenvalues.\n","    \"\"\"\n","    # TODO:\n","\n","\n","    if is_bipartite:\n","        print(\"The graph appears to be bipartite.\")\n","    else:\n","        print(\"The graph does not appear to be bipartite.\")\n","\n","    return is_bipartite, eigenvalues"]},{"cell_type":"code","execution_count":null,"id":"f8165974","metadata":{"id":"f8165974"},"outputs":[],"source":["# Spectral community structure analysis for G1\n","print(\"============ Analyzing Community Structure for G1 ============\")\n","has_comm_G1, num_comms_G1, lap_eigs_G1 = check_community_by_eigenvalues(G1)\n","\n","# Spectral community structure analysis for G2\n","print(\"============ Analyzing Community Structure for G2 ============\")\n","has_comm_G2, num_comms_G2, lap_eigs_G2 = check_community_by_eigenvalues(G2)\n","\n","# Spectral bipartite analysis for G1\n","print(\"============ Analyzing Bipartiteness for G1 ============\")\n","is_bip_G1, adj_eigs_G1 = check_bipartite_by_eigenvalues(G1)\n","\n","# Spectral bipartite analysis for G2\n","print(\"============ Analyzing Bipartiteness for G2 ============\")\n","is_bip_G2, adj_eigs_G2 = check_bipartite_by_eigenvalues(G2)"]},{"cell_type":"markdown","id":"b24f9491","metadata":{"id":"b24f9491"},"source":["### Task 2: Recover the block structure and visualize. (2.0 points)"]},{"cell_type":"markdown","id":"7c901371","metadata":{"id":"7c901371"},"source":["After determining the structure of each graph, reorder its nodes and generate a Dot Matrix Plot of the recovered adjacency matrix (similar to page 4 of the lecture of Graph Spectrum).\n","\n","You must implement the functionality of community structure based on Graph Spectrum theory. For bipartie graph, design an algorithm to recover its bipartite block structure. there is no expectation that such an algorithm has to use graph spectrum. *Hint:* for bipartite graphs, consider how to partition the nodes based on the definition of bipartiteness, you may need to use search algorithms you learned in the Algorithms course.\n","\n","In the returned dot matrix plot, the blocks should be ordered from left to right in ascending order of size. Also, return a list indicating the size (number of nodes) of each community or partition. After completing the code, you need to briefly explain why your algorithm is able to successfully recover the structure (you may use formulas to support your explanation if necessary). You are still not allowed to directly call any library functions that perform the reordering or structural recovery."]},{"cell_type":"code","execution_count":null,"id":"430ba982","metadata":{"id":"430ba982"},"outputs":[],"source":["def recover_blocks_community(G, num_communities, plot=True):\n","    \"\"\"\n","    Recover the block structure of a community graph G using spectral clustering.\n","\n","    Parameters:\n","      G: NetworkX graph assumed to have community structure.\n","      num_communities: The number of communities (blocks) to recover.\n","      plot: If True, plot the reordered adjacency matrix.\n","\n","    Returns:\n","      labels: Dictionary mapping each node to its community label.\n","      ordering: List of nodes reordered so that blocks are arranged in ascending order by block size.\n","      block_sizes: List of sizes (number of nodes) for each block in the ordering.\n","    \"\"\"\n","    # TODO:\n","\n","\n","    if plot:\n","        plot_adjacency_matrix(G, node_order=ordering,\n","                              title=\"Reordered Adjacency Matrix (Community Blocks Sorted by Size)\")\n","\n","    print(\"Community block structure recovered. Blocks sorted (ascending size):\")\n","    for i, size in enumerate(block_sizes):\n","        print(\"  Block {}: size {}\".format(i, size))\n","\n","    return labels, ordering, block_sizes"]},{"cell_type":"code","execution_count":null,"id":"7136014e","metadata":{"id":"7136014e"},"outputs":[],"source":["def recover_blocks_bipartite(G, plot=True):\n","    \"\"\"\n","    Recover the bipartite block structure of graph G by manually checking bipartiteness with search algorithm.\n","\n","    Parameters:\n","      G: NetworkX graph assumed to be bipartite.\n","      plot: If True, plot the reordered adjacency matrix.\n","\n","    Returns:\n","      labels: Dictionary mapping each node to its block ID (0, 1, 2, ...).\n","      ordering: List of nodes reordered so that blocks are arranged in ascending order by size.\n","      block_sizes: List of block sizes corresponding to the ordering.\n","\n","    If any connected component is not bipartite (e.g., due to an odd cycle), the function returns (None, None, None)\n","      and prints an error message.\n","    \"\"\"\n","    # TODO:\n","\n","\n","    if plot:\n","        plot_adjacency_matrix(G, node_order=ordering,\n","                              title=\"Reordered Adjacency Matrix (Bipartite Blocks Sorted by Size)\")\n","\n","    print(\"Bipartite block structure recovered. Blocks sorted (ascending size):\")\n","    for i, size in enumerate(block_sizes):\n","        print(\"  Block {}: size {}\".format(i, size))\n","\n","    return labels, ordering, block_sizes"]},{"cell_type":"code","execution_count":null,"id":"360812aa","metadata":{"id":"360812aa"},"outputs":[],"source":["# Recover block structure for community and bipartite graph\n","# TODO:\n","# Recover block structure for community graph. Replace 'i' with correct graph number\n","num_communities =\n","print(\"\\nRecovering block structure for community graph G_i...\")\n","comm_labels, comm_ordering, comm_block_sizes = recover_blocks_community(Gi, num_communities, plot=True)\n","\n","# Recover block structure for bipartite graph. Replace 'i' with correct graph number\n","print(\"\\nRecovering block structure for bipartite graph G_i...\")\n","bip_labels, bip_ordering, bip_block_sizes = recover_blocks_bipartite(Gi, plot=True)"]},{"cell_type":"markdown","id":"18ea8104","metadata":{"id":"18ea8104"},"source":["1. **Why your algorithm is able to successfully recover the structure?**"]},{"cell_type":"markdown","id":"351c0e02","metadata":{"id":"351c0e02"},"source":["TODO:"]},{"cell_type":"markdown","id":"5de9ffdc","metadata":{"id":"5de9ffdc"},"source":["### Task 3* (compulsory for COMP8880, optional for COMP4880) (1.0 points)"]},{"cell_type":"markdown","id":"68c03932","metadata":{"id":"68c03932"},"source":["We have explored bipartite graphs above. Now, we will take it a step further and work with *k-partite graphs*. A *k-partite graph* is defined as a graph whose nodes can be partitioned into *k disjoint sets*, such that:\n","- There are *no edges within each set* (i.e., each is an independent set),\n","- Edges may exist *between nodes in different sets*.\n","\n","Try to recover the block structure of the adjacency matrix of a shuffled *k-partite graph*, such that the blocks appear in ascending order of size from left to right. An example is shown in the `Figures/graph_comparison.png`. Also briefly explain your algorithm. You should implement the algorithm themselves and should not use library functions that directly implement the core logic.\n","\n","The graph you need to work with is provided in `Data/2_2/G3_nodes.txt` and `G3_edges.txt`, where the first column in `G3_nodes.txt` represents the node ID and the second column represents the color ID. You are not allowed to use the color ID for recovery. The color information is provided only to help you visually verify whether you have correctly recovered the components.\n","\n","*Hints*: you may need to use heuristic algorithm you learned in the Algorithms course."]},{"cell_type":"markdown","id":"c756cde7","metadata":{"id":"c756cde7"},"source":["<img src=\"Figures/graph_comparison.png\" alt=\"Image\" width=\"600\"/>"]},{"cell_type":"code","execution_count":null,"id":"41e4661c","metadata":{"id":"41e4661c"},"outputs":[],"source":["from Utils import read_graph_from_files_with_color, color_adjacency_matrix"]},{"cell_type":"code","execution_count":null,"id":"f9350575","metadata":{"id":"f9350575"},"outputs":[],"source":["def recover_graph(G):\n","    \"\"\"\n","    You may modify the definition of this function.\n","    \"\"\"\n","    # TODO:\n","\n","\n","    return A, k, color_vector"]},{"cell_type":"code","execution_count":null,"id":"3bba5d71","metadata":{"id":"3bba5d71"},"outputs":[],"source":["G3 = read_graph_from_files_with_color(\"Data/2_2/G3_nodes.txt\", \"Data/2_2/G3_edges.txt\")\n","A, k, color_vector = recover_graph(G3)\n","A_colored = color_adjacency_matrix(A, color_vector)\n","\n","plt.figure(figsize=(6, 6))\n","plt.imshow(A_colored.astype(np.uint8), interpolation='none')\n","plt.title(f\"Find k = {k}\")\n","plt.show()"]},{"cell_type":"markdown","id":"52ff440f","metadata":{"id":"52ff440f"},"source":["1. **Why your algorithm is able to successfully recover the structure?**"]},{"cell_type":"markdown","id":"c57accf2","metadata":{"id":"c57accf2"},"source":["TODO:"]},{"cell_type":"markdown","id":"6e85660b-4a7d-4d5f-a731-b84f5a993b9a","metadata":{"id":"6e85660b-4a7d-4d5f-a731-b84f5a993b9a"},"source":["---"]},{"cell_type":"markdown","id":"809bbad7-2d02-47ee-98bc-e8b3b03c64ce","metadata":{"id":"809bbad7-2d02-47ee-98bc-e8b3b03c64ce"},"source":["## Problem 2.3: Estimate point location from affinity matrices (3.5 points)"]},{"cell_type":"markdown","id":"4a1dd481-1a80-4808-8487-ddc973f0ee7c","metadata":{"id":"4a1dd481-1a80-4808-8487-ddc973f0ee7c"},"source":["In this problem, you will be given the affinity matrices `A` and `B` which are generated by 100 2D data points. Your task is to recover the exact coordinates of these data points."]},{"cell_type":"markdown","id":"97df988d-506c-4efc-8e68-0c7599472166","metadata":{"id":"97df988d-506c-4efc-8e68-0c7599472166"},"source":["### Task 1: Fully obeserved affinity matrix (1.5 points)\n","In this task, we consider a complete $n\\times n$ affinity matrix $A$ with $A_{i,j} = \\exp\\left(-\\Vert x_i - x_j \\Vert_2^2\\right)$. In the following block, we will implement the recovery procedure for the problem. `A` is an $200\\times 200$ affinity matrix defined as above. You are given the exact coordinates of two points $x_1 = (0.1,0.5), x_2 = (-0.2, -0.2), x_3 = (0.3, 0.2)$. Your goal is to recover the remaining 197 points.\n","\n","Hint:\n","1. Prove that $\\Vert x_1 - x_i \\Vert_2^2 + \\Vert x_1 - x_j \\Vert_2^2 - \\Vert x_i - x_j \\Vert_2^2 = 2(x_1 - x_i)^T(x_1 - x_j)$ (The proof of this identity should be presented in your submission if you used this identity to recover the data points).\n","2. [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) is useful once you have some $A = X^TX$ and you want to recover $X$. Since A may not be positive definite, you may use [spectral decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Eigendecomposition_of_a_matrix) with `np.linalg.eig` instead of `np.chelosky`.\n","3. You can use the given coordinates $x_1, x_2, x_3$ to fix the coordinates of all data points. Specifically, suppose we have a set of data points $\\hat{x_1}, \\hat{x_2}, \\ldots, \\hat{x_n}$ also constitutes to the affinity matrix `A`. You can solve the affine transformation that transforms $x_1$ to $\\hat{x_1}$, $x_2$ to $\\hat{x_2}$ and $x_3$ to $\\hat{x_3}$.\n","\n","Your tasks:\n","1. Describe your approach in mathematical arguments.\n","2. Implement your appraoch by completing `A`."]},{"cell_type":"markdown","id":"5cd646e2-b742-4265-a1af-6382a1056a40","metadata":{"id":"5cd646e2-b742-4265-a1af-6382a1056a40"},"source":["TODO"]},{"cell_type":"code","execution_count":null,"id":"afe33289-2d9c-44ae-87b0-da2a4808b782","metadata":{"id":"afe33289-2d9c-44ae-87b0-da2a4808b782"},"outputs":[],"source":["# data loading\n","A = np.load(\"./Data/2_3/A.npy\")\n","\n","def recover_full(A) -> np.ndarray:\n","    # TODO\n","\n","\n","    # Make sure your output is 200*2\n","    return np.zeros((200,2))\n","X =  recover_full(A)\n","plt.scatter(X[:,0], X[:,1])"]},{"cell_type":"markdown","id":"ae2a58ac-bb49-4677-95ee-a43af9e03375","metadata":{"id":"ae2a58ac-bb49-4677-95ee-a43af9e03375"},"source":["### Task 2: Incomplete affinity matrix - Gradient calculation (0.5 points)\n","Similar to task 1, you are given an $200 \\times 200$ affinity matrix $B$. However, most of the entries in `B` are missing and the missing entries are replaced by value `-1` (around 15000 entries in `B` are missing). Again, your task is to infer the coordinates of the data points which generate the affinity matrix `B`. Here we adopt an optimisation-based approach. Denoting the data points as $\\{x_1, \\ldots, x_n \\}$, we will try to minimise:\n","$$ \\min_{x_1, \\ldots, x_n} L(x_1,\\ldots, x_n) = \\sum_{(i,j)\\in \\mathcal{O}} \\left(\\exp\\left(-\\Vert x_i - x_j \\Vert_2^2\\right) - B_{i,j}\\right)^2, $$\n","where $\\mathcal{O}$ is the set of indices for which the entries can be observed. It is the least square loss of the given affinity matrix and the (observed parts of) affinity matrix generated by the recovered coordinates. The optimal solution could be solved by iterated algorithms such as gradient descent. To start with, we first compute the gradient $\\nabla_X L$ where $X$ is the data matrix where the $i$-th row represents the $x_i$. The gradient should also be a $n\\times 2$ matrix with the $i$-th row to be the gradient $\\nabla_{x_i} L$. First, calculate the closed-form of $\\nabla_{x_i} L$."]},{"cell_type":"markdown","id":"2a1d68a6-9e44-4ca4-a173-5b7c98ff5796","metadata":{"id":"2a1d68a6-9e44-4ca4-a173-5b7c98ff5796"},"source":["TODO"]},{"cell_type":"markdown","id":"96a6286e-e144-453d-a0e4-786ade25a95a","metadata":{"id":"96a6286e-e144-453d-a0e4-786ade25a95a"},"source":["### Task 3: Incomplete affinity matrix - Implementation (1.5 points)\n","In this task, we will implement the above optimisation procedure for the data recovery problem. You will be given an initial guess `X0` of the data points. You may use `X0` as the initialiser of your optimisation solver. Your tasks:\n","- Complete the function `loss_partial`, which outputs the value of loss function $L^m$.\n","- Complete the function `gradient_partial`, which outputs $\\nabla_{X} L^m$\n","- Complete the function `recover_partial`, which outputs the inferred coordinates of the data points. You may use [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) for the optimisation solver.\n","\n","Hints:\n","- The data points could jointly form a closed shape. You may use this to check the correctness of your answer or design regularisers of the above problem.\n","- The coordinates of $x_1$ and $x_2$ are used to fix all the data points to a particular location and orientation. You may set the first two rows of the gradients as $0$ and set the initial point accordingly. Or, you can recalculate the affine transformation after the data points have been inferred.\n","- `scipy.optimize.minimize` only accepts $n\\times 1$ vectors (it does not accept a matrix to be the variable). You may need to flatten the gradient we calculated above into a vector.\n","\n","Note: We do not expect you to reconstruct the exact solution based on this method. Marks will be given solely based on the correctness of your implementation."]},{"cell_type":"code","execution_count":null,"id":"8e8e12d2-1cd3-4d64-80c2-8fdbec6810fb","metadata":{"id":"8e8e12d2-1cd3-4d64-80c2-8fdbec6810fb"},"outputs":[],"source":["B = np.load(\"./Data/2_3/B.npy\")\n","X0 = np.load(\"./Data/2_3/X0.npy\")\n","mask = B > 0\n","print(mask.sum())\n","\n","def loss_partial(X, B, mask) -> float:\n","    #Placeholder for the answer\n","    L = 0\n","    #TODO\n","\n","    return L\n","\n","def gradient_partial(X, B, mask) -> np.ndarray:\n","    # Placeholder for the answer\n","    grad = np.zeros(np.shape(X))\n","    #TODO\n","\n","    return grad\n","\n","\n","def recover_partial(B) -> np.ndarray:\n","    # Placeholder for the answer\n","    X = np.zeros((200,2))\n","\n","    #The output X should be in the shape (n,2) to make sure the plotting rountine works.\n","    return X\n","\n","\n","plt.title(\"Q2.3 data visualisation -- Task 3\")\n","X = recover_partial(B)\n","plt.scatter(X[:,0], X[:,1])\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}